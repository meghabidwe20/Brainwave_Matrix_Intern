# -*- coding: utf-8 -*-
"""Twittter Sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1log0wchRehWjpZV31crcmALvucrYacDU
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
import warnings
# %matplotlib inline
warnings.filterwarnings('ignore')

"""# **Loading the Dataset **"""

df = pd.read_csv('twitter.csv')
df.head()

# datatype  info
df.info()

"""# **Preprocessing the Dataset**"""

#removes pattern in the input text

def remove_pattern(input_txt , pattern):
  r = re.findall(pattern, input_txt)
  for word in r:
    input_txt = re.sub(word, "", input_txt)
  return input_txt

# remove twitter handles(@user)
df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], "@[\w]*")
df.head()

# remove special charaters , number & punctucations
df['clean_tweet']=df['clean_tweet'].str.replace("[^a-zA-Z#]", " ")
df.head()

#remove short words
df['clean_tweet'] = df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
df.head()

#individual words considered as takens
tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())
tokenized_tweet.head()

#stem the words
from nltk.stem.porter import *
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])
tokenized_tweet.head()

#combine into a single
for i in range(len(tokenized_tweet)):
  tokenized_tweet[i] = ' '.join(tokenized_tweet[i])

df['clean_tweet'] = tokenized_tweet
df.head()

"""# **Exploratory Data Analysis**"""

# !pip install wordcloud

#visualize the frequent words
all_words = " ".join([sentence for sentence in df['clean_tweet']])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

#plot the graph
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# frequent words visualization for +ve
all_words = " ".join([sentence for sentence in df['clean_tweet'][df['label']==0]])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

# plot the graph
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#extract the hashtag
def hashtag_extract(tweets):
  hashtags = []
  #loop  words in the tweet
  for word in tweets:
    ht = re.findall(r"#(\w+)", word)
    hashtags.append(ht)


  return hashtags

#extract hashtag froms non racist/sexist tweets
ht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])

#extarct hashtag from racist/sexist tweets
ht_positive = hashtag_extract(df['clean_tweet'][df['label']==1])

ht_positive[:5]

#extract hashtag froms non racist/sexist tweets
ht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])

#extarct hashtag from racist/sexist tweets
ht_negative = hashtag_extract(df['clean_tweet'][df['label']==1]) # assign to ht_negative

ht_positive[:5]

#unnest list
ht_positive = (ht_positive) # remove sum and extra bracket
ht_negative = (ht_negative) # remove sum and extra bracket

ht_positive[:5]

#extract hashtag froms non racist/sexist tweets
ht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])

#extarct hashtag from racist/sexist tweets
ht_negative = hashtag_extract(df['clean_tweet'][df['label']==1])


#unnest list
ht_positive = [item for sublist in ht_positive for item in sublist]
ht_negative = [item for sublist in ht_negative for item in sublist]

freq = nltk.FreqDist(ht_positive)
d = pd.DataFrame({'Hashtag': list(freq.keys()),
                  'Count': list(freq.values())})
d.head()

#select top 10 hashtag
d = d.nlargest(columns='Count', n=10)
plt.figure(figsize=(16,5))
ax = sns.barplot(data=d, x='Hashtag', y='Count')
ax.set(ylabel='Count')
plt.show()

freq = nltk.FreqDist(ht_negative)
d = pd.DataFrame({'Hashtag': list(freq.keys()),
                  'Count': list(freq.values())})
d.head()

#select top 10 hashtag
d = d.nlargest(columns='Count', n=10)
plt.figure(figsize=(16,5))
ax = sns.barplot(data=d, x='Hashtag', y='Count')
ax.set(ylabel='Count')
plt.show()

"""# Input **Split**"""

#feature extraction
from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
bow = bow_vectorizer.fit_transform(df['clean_tweet'])

bow[0].toarray()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(bow, df['label'], random_state=42, test_size=0.25)

"""# **Model Training**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score

#training
model = LogisticRegression()
model.fit(x_train, y_train)

#testing
pred = model.predict(x_test)
f1_score(y_test, pred)

accuracy_score(y_test, pred)

#use probablity to get output
pred_prob = model.predict_proba(x_test)
pred = pred_prob[:,1] >= 0.3
pred = pred.astype(int)

f1_score(y_test, pred)

accuracy_score(y_test, pred)

pred_prob[0][1]>=0.3